{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h4>Diplomatura en CDAAyA 2019 - FaMAF - UNC</h4>\n",
    "<h1>Predicción del Nivel de Pobreza de Hogares en Costa Rica</h1>\n",
    "<h3>Aprendizaje Automático Supervisado</h3>\n",
    "</center>\n",
    "</left>\n",
    "<h4>Julieta Bergamasco</h4>\n",
    "</left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción\n",
    "\n",
    "En la siguiente notebook se presentará la consigna a seguir para el cuarto práctico del proyecto, correspondiente a la materia Aprendizaje Automático Supervisado. El objetivo consiste en profundizar en la aplicación de métodos de aprendizaje supervisado aprendidos en el curso, así como también en métodos de _ensemble learning_. Esto, siempre a través de experimentos reproducibles y evaluando a su vez la conveniencia de uno u otro, así como la selección de diferentes hiperparámetros a partir del cálculo de las métricas pertinentes.\n",
    "\n",
    "A los fines de este práctico, consideraremos el problema original de nuestro proyecto, el cual consiste en un problema de clasificación múltiple con datos etiquetados. Nuevamente, al igual que en el práctico anterior, será importante evaluar el desbalance de clases y qué decisiones tomaremos al respecto.\n",
    "\n",
    "Para ello, comenzaremos con las importaciones pertinentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow\n",
      "  Using cached https://files.pythonhosted.org/packages/95/eb/91b42c744e75e4523b29ca8d746a43fddb9def239adae5d5071e4044c760/mlflow-1.3.0.tar.gz\n",
      "Collecting alembic (from mlflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/6f/42/48447bf41287bc577e4f340e7c28578e322567f5622a915bdfa01c83dc76/alembic-1.2.1.tar.gz\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\soluj\\anaconda3\\lib\\site-packages (from mlflow) (7.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\soluj\\anaconda3\\lib\\site-packages (from mlflow) (0.8.0)\n",
      "Collecting databricks-cli>=0.8.7 (from mlflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/51/0b/75dac581d98c493be74df97f3ea515c678da2e4be8cafbaf9cba9f01c309/databricks-cli-0.9.0.tar.gz\n",
      "Requirement already satisfied: requests>=2.17.3 in c:\\users\\soluj\\anaconda3\\lib\\site-packages (from mlflow) (2.21.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\soluj\\anaconda3\\lib\\site-packages (from mlflow) (1.12.0)\n",
      "Requirement already satisfied: Flask in c:\\users\\soluj\\anaconda3\\lib\\site-packages (from mlflow) (1.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\soluj\\anaconda3\\lib\\site-packages (from mlflow) (1.16.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\soluj\\anaconda3\\lib\\site-packages (from mlflow) (0.24.2)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\soluj\\anaconda3\\lib\\site-packages (from mlflow) (2.8.0)\n",
      "Collecting protobuf>=3.6.0 (from mlflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/a8/ae/a11b9b0c8e2410b11887881990b71f54ec39b17c4de2b5d850ef66aade8c/protobuf-3.10.0-cp37-cp37m-win_amd64.whl (1.0MB)\n",
      "Collecting gitpython>=2.1.0 (from mlflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/ff/f8/05f58bd7852dad7edcf70a8de953b4fa39f61cdc13812ae62118be6ffa23/GitPython-3.0.3-py3-none-any.whl\n",
      "Requirement already satisfied: pyyaml in c:\\users\\soluj\\anaconda3\\lib\\site-packages (from mlflow) (5.1)\n",
      "Collecting querystring_parser (from mlflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/4a/fa/f54f5662e0eababf0c49e92fd94bf178888562c0e7b677c8941bbbcd1bd6/querystring_parser-1.2.4.tar.gz\n",
      "Collecting simplejson (from mlflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/e3/24/c35fb1c1c315fc0fffe61ea00d3f88e85469004713dab488dee4f35b0aff/simplejson-3.16.0.tar.gz\n",
      "Collecting docker>=4.0.0 (from mlflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/cc/ca/699d4754a932787ef353a157ada74efd1ceb6d1fc0bfb7989ae1e7b33111/docker-4.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: entrypoints in c:\\users\\soluj\\anaconda3\\lib\\site-packages (from mlflow) (0.3)\n",
      "Collecting sqlparse (from mlflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/ef/53/900f7d2a54557c6a37886585a91336520e5539e3ae2423ff1102daf4f3a7/sqlparse-0.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\soluj\\anaconda3\\lib\\site-packages (from mlflow) (1.3.1)\n",
      "Collecting gorilla (from mlflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/e3/56/5a683944cbfc77e429c6f03c636ca50504a785f60ffae91ddd7f5f7bb520/gorilla-0.3.0-py2.py3-none-any.whl\n",
      "Collecting waitress (from mlflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/2d/0b/84a7da7fd12ee556dd193c24b635a11b840d314bc580771dd2a3151450c9/waitress-1.3.1-py2.py3-none-any.whl\n",
      "Collecting Mako (from alembic->mlflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/b0/3c/8dcd6883d009f7cae0f3157fb53e9afb05a0d3d33b3db1268ec2e6f4a56b/Mako-1.1.0.tar.gz\n",
      "Collecting python-editor>=0.3 (from alembic->mlflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
      "Collecting tabulate>=0.7.7 (from databricks-cli>=0.8.7->mlflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/66/d4/977fdd5186b7cdbb7c43a7aac7c5e4e0337a84cb802e154616f3cfc84563/tabulate-0.8.5.tar.gz\n",
      "Collecting configparser>=0.3.5 (from databricks-cli>=0.8.7->mlflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/7a/2a/95ed0501cf5d8709490b1d3a3f9b5cf340da6c433f896bbe9ce08dbe6785/configparser-4.0.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\soluj\\anaconda3\\lib\\site-packages (from requests>=2.17.3->mlflow) (1.24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\soluj\\anaconda3\\lib\\site-packages (from requests>=2.17.3->mlflow) (2019.3.9)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\soluj\\anaconda3\\lib\\site-packages (from requests>=2.17.3->mlflow) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\soluj\\anaconda3\\lib\\site-packages (from requests>=2.17.3->mlflow) (3.0.4)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in c:\\users\\soluj\\anaconda3\\lib\\site-packages (from Flask->mlflow) (1.1.0)\n",
      "Requirement already satisfied: Jinja2>=2.10 in c:\\users\\soluj\\anaconda3\\lib\\site-packages (from Flask->mlflow) (2.10)\n",
      "Requirement already satisfied: Werkzeug>=0.14 in c:\\users\\soluj\\anaconda3\\lib\\site-packages (from Flask->mlflow) (0.14.1)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\soluj\\anaconda3\\lib\\site-packages (from pandas->mlflow) (2018.9)\n",
      "Requirement already satisfied: setuptools in c:\\users\\soluj\\anaconda3\\lib\\site-packages (from protobuf>=3.6.0->mlflow) (40.8.0)\n",
      "Collecting gitdb2>=2.0.0 (from gitpython>=2.1.0->mlflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/03/6c/99296f89bad2ef85626e1df9f677acbee8885bb043ad82ad3ed4746d2325/gitdb2-2.0.6-py2.py3-none-any.whl\n",
      "Collecting pypiwin32==223; sys_platform == \"win32\" and python_version >= \"3.6\" (from docker>=4.0.0->mlflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/d0/1b/2f292bbd742e369a100c91faa0483172cd91a1a422a6692055ac920946c5/pypiwin32-223-py3-none-any.whl\n",
      "Collecting websocket-client>=0.32.0 (from docker>=4.0.0->mlflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\soluj\\anaconda3\\lib\\site-packages (from Mako->alembic->mlflow) (1.1.1)\n",
      "Collecting smmap2>=2.0.0 (from gitdb2>=2.0.0->gitpython>=2.1.0->mlflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/55/d2/866d45e3a121ee15a1dc013824d58072fd5c7799c9c34d01378eb262ca8f/smmap2-2.0.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pywin32>=223 in c:\\users\\soluj\\anaconda3\\lib\\site-packages (from pypiwin32==223; sys_platform == \"win32\" and python_version >= \"3.6\"->docker>=4.0.0->mlflow) (223)\n",
      "Building wheels for collected packages: mlflow, alembic, databricks-cli, querystring-parser, simplejson, Mako, tabulate\n",
      "  Building wheel for mlflow (setup.py): started\n",
      "  Building wheel for mlflow (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\soluj\\AppData\\Local\\pip\\Cache\\wheels\\f1\\e0\\1c\\663c9b2bb00f32c235c98a43c39d81c2a25544a889a32649ea\n",
      "  Building wheel for alembic (setup.py): started\n",
      "  Building wheel for alembic (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\soluj\\AppData\\Local\\pip\\Cache\\wheels\\c6\\b8\\fd\\1f16371156a8184172c4935cbbef6a345d57dd447e31a36633\n",
      "  Building wheel for databricks-cli (setup.py): started\n",
      "  Building wheel for databricks-cli (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\soluj\\AppData\\Local\\pip\\Cache\\wheels\\e8\\ce\\cd\\9d2214455ef53a4d9cdbca640dc27828a6c88cd5ef7d0c04de\n",
      "  Building wheel for querystring-parser (setup.py): started\n",
      "  Building wheel for querystring-parser (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\soluj\\AppData\\Local\\pip\\Cache\\wheels\\1e\\41\\34\\23ebf5d1089a9aed847951e0ee375426eb4ad0a7079d88d41e\n",
      "  Building wheel for simplejson (setup.py): started\n",
      "  Building wheel for simplejson (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\soluj\\AppData\\Local\\pip\\Cache\\wheels\\5d\\1a\\1e\\0350bb3df3e74215cd91325344cc86c2c691f5306eb4d22c77\n",
      "  Building wheel for Mako (setup.py): started\n",
      "  Building wheel for Mako (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\soluj\\AppData\\Local\\pip\\Cache\\wheels\\98\\32\\7b\\a291926643fc1d1e02593e0d9e247c5a866a366b8343b7aa27\n",
      "  Building wheel for tabulate (setup.py): started\n",
      "  Building wheel for tabulate (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\soluj\\AppData\\Local\\pip\\Cache\\wheels\\e1\\41\\5e\\e201f95d90fc84f93aa629b6638adacda680fe63aac47174ab\n",
      "Successfully built mlflow alembic databricks-cli querystring-parser simplejson Mako tabulate\n",
      "Installing collected packages: Mako, python-editor, alembic, tabulate, configparser, databricks-cli, protobuf, smmap2, gitdb2, gitpython, querystring-parser, simplejson, pypiwin32, websocket-client, docker, sqlparse, gorilla, waitress, mlflow\n",
      "Successfully installed Mako-1.1.0 alembic-1.2.1 configparser-4.0.2 databricks-cli-0.9.0 docker-4.1.0 gitdb2-2.0.6 gitpython-3.0.3 gorilla-0.3.0 mlflow-1.3.0 protobuf-3.10.0 pypiwin32-223 python-editor-1.0.4 querystring-parser-1.2.4 simplejson-3.16.0 smmap2-2.0.5 sqlparse-0.3.0 tabulate-0.8.5 waitress-1.3.1 websocket-client-0.56.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de las librerías necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import store\n",
    "\n",
    "\n",
    "# Puede que nos sirvan también\n",
    "import matplotlib as mpl\n",
    "mpl.get_cachedir()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "# import sklearn as skl\n",
    "import store\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error, classification_report, roc_curve, auc\n",
    "from sklearn import ensemble\n",
    "from sklearn import svm\n",
    "from sklearn import neural_network\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from ml.visualization import plot_confusion_matrix, plot_learning_curve\n",
    "import datetime\n",
    "import mlflow\n",
    "np.random.seed(0)  # Para mayor determinismo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 150)\n",
    "pd.set_option('display.max_rows', 150)\n",
    "pd.set_option('max_colwidth', 151)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consigna para Aprendizaje Automático Supervisado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Preprocesamiento\n",
    "\n",
    "A los fines de realizar este práctico, se utilizará el dataset original, pero descartando todas aquellas columnas que se hayan calculado en base a features preexistentes. La división entre train y test será realizada en este mismo práctico.\n",
    "A continuación se detallan los pasos a seguir para el preprocesamiento de los datos, prácticamente iguales a los del práctico anterior.\n",
    "\n",
    "#### 1. Obtención del Dataset\n",
    "\n",
    "Cargar el conjunto de entrenamiento original. Luego, eliminar las columnas calculadas en base a features preexistentes.\n",
    "\n",
    "#### 2. Aplicar Script de Curación\n",
    "\n",
    "Inicialmente, con el objetivo de preparar los datos que alimentarán los modelos de aprendizaje automático (ML) propuestos, deberán aplicar el script de curación obtenido en el segundo práctico.\n",
    "En esta etapa, nuevamente, pueden adicionar los atributos que crean pertinentes a priori o que hayan encontrado interesantes por tener mayor correlación con la variable `Target`. Por ejemplo, la que crearon de aparatos tecnológicos.\n",
    "\n",
    "#### 3. Análisis del Balance de Clases\n",
    "\n",
    "¿Cómo luce el balance de clases? ¿Tomarán alguna decisión al respecto?\n",
    "\n",
    "#### 4. Multicolinealidad Exacta y Variables Dummies\n",
    "\n",
    "Las variables explicativas no deben estar muy correlacionadas entre ellas, ya que la variabilidad de una y otra estarán explicando la misma parte de variabilidad de la variable dependiente. Esto es lo que se conoce como multicolinealidad, lo cual deriva en la imposibilidad de estimar los parámetros cuando la misma es exacta o en estimaciones muy imprecisas cuando la misma es aproximada.\n",
    "\n",
    "Dadas las características de nuestro dataset, nos encontramos con múltiples variables dummies. Analizar en qué casos resulta pertinente descartar la k-ésima dummie de cada categoría.\n",
    "\n",
    "#### 5. Clasificación a Nivel Hogar o a Nivel Individuo\n",
    "\n",
    "A partir de análisis previos, sabemos que en nuestro problema la clasificación corresponde al hogar (y no al individuo). Es decir, cada hogar tendrá una única etiqueta. Por lo tanto, para aplicar los modelos, se presentan las siguientes opciones:\n",
    "\n",
    "a. Quedarse sólo con los individuos jefes o jefas de hogar, y de éstos, con todas las variables asociadas que se repiten para el hogar. Asimismo, crear variables con medidas resumen para todas las variables que no se repitan por hogar.\n",
    "\n",
    "b. Mantener el dataset a nivel individuo.\n",
    "\n",
    "Del siguiente punto en adelante, **deberán resolver para el dataset que consideren más apropiado**, en base a los resultados obtenidos en el práctico anterior.\n",
    "\n",
    "#### 6. Normalización de Atributos\n",
    "\n",
    "Es posible que sea necesario normalizar las features de nuestro dataset, dado que muchos de los algoritmos de clasificación supervisada lo requieren. ¿En qué casos tendrá que implementarse normalización, considerando los nuevos modelos propuestos?\n",
    "\n",
    "Aplicar al dataset la normalización de atributos que consideren adecuada.\n",
    "\n",
    "#### 7. Mezca Aleatória y División en Train/Test\n",
    "\n",
    "Finalmente, están en condiciones de **dividir el dataset en Train y Test**, utilizando para este último conjunto un 20% de los datos disponibles. Previo a esta división, es recomendable que mezclen los datos aleatoriamente.\n",
    "De este modo, deberán obtener cuatro conjuntos de datos, para cada uno de los datasets: ```X_train```, ```X_test```, ```y_train``` y ```y_test```.\n",
    "\n",
    "---\n",
    "\n",
    "A modo de ayuda, **en esta notebook encontrarán una especie de template** que sigue los pasos propuestos y que deberán ir completando.\n",
    "\n",
    "Recuerden que la ciencia de datos es un **proceso circular, continuo y no lineal**. Es decir, si los datos requieren de mayor procesamiento para satisfacer las necesidades de algoritmos de ML (cualesquiera de ellos), vamos a volver a la etapa inicial para, por ejemplo, crear nuevas features, tomar decisiones diferentes sobre valores faltantes o valores atípicos (outliers), descartar features, entre otras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Aplicación de Modelos de Clasificación de Aprendizaje Automático Supervisado\n",
    "\n",
    "Una vez finalizada la etapa de preprocesamiento, se propone implementar diferentes modelos de clasificación **para el dataset seleccionado**, utilizando la librería Scikit-Learn (o la que consideren apropiada):\n",
    "\n",
    "1. Support Vector Machines (SVM), probando clasificadores con distintos kernels.\n",
    "2. Random Forest, utilizando parámetros de normalización cuando lo crean pertinente.\n",
    "3. Red neuronal.\n",
    "\n",
    "Para cada uno de ellos, se pide responder las siguientes consignas:\n",
    "- Utilizar dos features para graficar las clases y la frontera de decisión, siempre que sea posible.\n",
    "- Agregar vector de Bias, cuando lo crean pertinente. Cuándo hace falta y cuándo no? Por qué?\n",
    "\n",
    "De estos tres modelos, cuál creen que es el más adecuado para nuestro caso de aplicación?\n",
    "\n",
    "Finalmente, **combinar los modelos en un clasificador por votos** (pueden implementar VotingClassifier, del módulo de ensemble).\n",
    "\n",
    "**Elegir el modelo que consideren que mejor aplica a nuestro problema.** Para ello, recuerden que los pasos a seguir en la selección pueden esquematizarse como sigue:\n",
    "\n",
    "#### 1. Descripción de la Hipótesis\n",
    "\n",
    "¿Cuál es nuestro problema? ¿Cómo se caracteriza? ¿Cuál es la hipótesis?\n",
    "\n",
    "#### 2. Selección de Regularizador\n",
    "\n",
    " ¿Utilizarán algún regularizador?¿Cuál?\n",
    "\n",
    "#### 3. Selección de Función de Costo\n",
    "\n",
    "¿Cuál será la función de costo utilizada?\n",
    "\n",
    "#### 4. Justificación de las Selecciones\n",
    "\n",
    "¿Por qué eligieron el modelo, el regularizador y la función de costo previas?\n",
    "\n",
    "Finalmente, para el modelo selecionado:\n",
    "\n",
    "- Utilizar el método *Grid Search*, o de búsqueda exahustiva, con *cross-validation* para profundizar en la búsqueda y selección de hiperparámetros (fine tuning).\n",
    "- Calcular métricas sobre el conjunto de entrenamiento y de evaluación para los mejores parámetros obtenidos:\n",
    "    + Accuracy o exactitud\n",
    "    + Reporte de clasificación\n",
    "    + Confusion matrix o matriz de confusión (graficar como heatmap)\n",
    "    + Curva ROC y área bajo la curva (AUC).\n",
    "- Es apropiado utilizar la métrica **acuracy o exactitud** como medida de performance de nuestro modelo, de acuerdo a los datos con los que contamos? Por qué?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entregables\n",
    "\n",
    "El entregable de este práctico consiste en **esta misma Notebook**, pero con el preprocesamiento aplicado y los modelos implementados, agregando las explicaciones que crean pertinentes y las decisiones tomadas, en caso de corresponder.\n",
    "\n",
    "Además, deberán sintetizar las principales conclusiones de éste y el anterior práctico en un PPT (dataset utilizado para entrenar el modelo, modelo más apropiado, selección de hiperparámetros y métricas calculadas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Preprocesamiento\n",
    "\n",
    "### 1. Carga de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comenzar, importamos los datos que vamos a utilizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>v2a1</th>\n",
       "      <th>hacdor</th>\n",
       "      <th>rooms</th>\n",
       "      <th>hacapo</th>\n",
       "      <th>v14a</th>\n",
       "      <th>refrig</th>\n",
       "      <th>v18q</th>\n",
       "      <th>v18q1</th>\n",
       "      <th>r4h1</th>\n",
       "      <th>r4h2</th>\n",
       "      <th>r4h3</th>\n",
       "      <th>r4m1</th>\n",
       "      <th>r4m2</th>\n",
       "      <th>r4m3</th>\n",
       "      <th>r4t1</th>\n",
       "      <th>r4t2</th>\n",
       "      <th>r4t3</th>\n",
       "      <th>tamhog</th>\n",
       "      <th>tamviv</th>\n",
       "      <th>escolari</th>\n",
       "      <th>rez_esc</th>\n",
       "      <th>hhsize</th>\n",
       "      <th>paredblolad</th>\n",
       "      <th>paredzocalo</th>\n",
       "      <th>paredpreb</th>\n",
       "      <th>pareddes</th>\n",
       "      <th>paredmad</th>\n",
       "      <th>paredzinc</th>\n",
       "      <th>paredfibras</th>\n",
       "      <th>paredother</th>\n",
       "      <th>pisomoscer</th>\n",
       "      <th>pisocemento</th>\n",
       "      <th>pisoother</th>\n",
       "      <th>pisonatur</th>\n",
       "      <th>pisonotiene</th>\n",
       "      <th>pisomadera</th>\n",
       "      <th>techozinc</th>\n",
       "      <th>techoentrepiso</th>\n",
       "      <th>techocane</th>\n",
       "      <th>techootro</th>\n",
       "      <th>cielorazo</th>\n",
       "      <th>abastaguadentro</th>\n",
       "      <th>abastaguafuera</th>\n",
       "      <th>abastaguano</th>\n",
       "      <th>public</th>\n",
       "      <th>planpri</th>\n",
       "      <th>noelec</th>\n",
       "      <th>coopele</th>\n",
       "      <th>sanitario1</th>\n",
       "      <th>sanitario2</th>\n",
       "      <th>sanitario3</th>\n",
       "      <th>sanitario5</th>\n",
       "      <th>sanitario6</th>\n",
       "      <th>energcocinar1</th>\n",
       "      <th>energcocinar2</th>\n",
       "      <th>energcocinar3</th>\n",
       "      <th>energcocinar4</th>\n",
       "      <th>elimbasu1</th>\n",
       "      <th>elimbasu2</th>\n",
       "      <th>elimbasu3</th>\n",
       "      <th>elimbasu4</th>\n",
       "      <th>elimbasu5</th>\n",
       "      <th>elimbasu6</th>\n",
       "      <th>epared1</th>\n",
       "      <th>epared2</th>\n",
       "      <th>epared3</th>\n",
       "      <th>etecho1</th>\n",
       "      <th>etecho2</th>\n",
       "      <th>etecho3</th>\n",
       "      <th>eviv1</th>\n",
       "      <th>eviv2</th>\n",
       "      <th>eviv3</th>\n",
       "      <th>dis</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "      <th>estadocivil1</th>\n",
       "      <th>estadocivil2</th>\n",
       "      <th>estadocivil3</th>\n",
       "      <th>estadocivil4</th>\n",
       "      <th>estadocivil5</th>\n",
       "      <th>estadocivil6</th>\n",
       "      <th>estadocivil7</th>\n",
       "      <th>parentesco1</th>\n",
       "      <th>parentesco2</th>\n",
       "      <th>parentesco3</th>\n",
       "      <th>parentesco4</th>\n",
       "      <th>parentesco5</th>\n",
       "      <th>parentesco6</th>\n",
       "      <th>parentesco7</th>\n",
       "      <th>parentesco8</th>\n",
       "      <th>parentesco9</th>\n",
       "      <th>parentesco10</th>\n",
       "      <th>parentesco11</th>\n",
       "      <th>parentesco12</th>\n",
       "      <th>idhogar</th>\n",
       "      <th>hogar_nin</th>\n",
       "      <th>hogar_adul</th>\n",
       "      <th>hogar_mayor</th>\n",
       "      <th>hogar_total</th>\n",
       "      <th>dependency</th>\n",
       "      <th>edjefe</th>\n",
       "      <th>edjefa</th>\n",
       "      <th>meaneduc</th>\n",
       "      <th>instlevel1</th>\n",
       "      <th>instlevel2</th>\n",
       "      <th>instlevel3</th>\n",
       "      <th>instlevel4</th>\n",
       "      <th>instlevel5</th>\n",
       "      <th>instlevel6</th>\n",
       "      <th>instlevel7</th>\n",
       "      <th>instlevel8</th>\n",
       "      <th>instlevel9</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>overcrowding</th>\n",
       "      <th>tipovivi1</th>\n",
       "      <th>tipovivi2</th>\n",
       "      <th>tipovivi3</th>\n",
       "      <th>tipovivi4</th>\n",
       "      <th>tipovivi5</th>\n",
       "      <th>computer</th>\n",
       "      <th>television</th>\n",
       "      <th>mobilephone</th>\n",
       "      <th>qmobilephone</th>\n",
       "      <th>lugar1</th>\n",
       "      <th>lugar2</th>\n",
       "      <th>lugar3</th>\n",
       "      <th>lugar4</th>\n",
       "      <th>lugar5</th>\n",
       "      <th>lugar6</th>\n",
       "      <th>area1</th>\n",
       "      <th>area2</th>\n",
       "      <th>age</th>\n",
       "      <th>SQBescolari</th>\n",
       "      <th>SQBage</th>\n",
       "      <th>SQBhogar_total</th>\n",
       "      <th>SQBedjefe</th>\n",
       "      <th>SQBhogar_nin</th>\n",
       "      <th>SQBovercrowding</th>\n",
       "      <th>SQBdependency</th>\n",
       "      <th>SQBmeaned</th>\n",
       "      <th>agesq</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_279628684</td>\n",
       "      <td>190000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21eb7fcc1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>10</td>\n",
       "      <td>no</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>100</td>\n",
       "      <td>1849</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1849</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_f29eb3ddd</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0e5d7a658</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>no</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>144</td>\n",
       "      <td>4489</td>\n",
       "      <td>1</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>64.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>4489</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_68de51c94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2c7317ea8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>no</td>\n",
       "      <td>11</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>121</td>\n",
       "      <td>8464</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>64.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>8464</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_d671db89c</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2b58d945f</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>11</td>\n",
       "      <td>no</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>81</td>\n",
       "      <td>289</td>\n",
       "      <td>16</td>\n",
       "      <td>121</td>\n",
       "      <td>4</td>\n",
       "      <td>1.777778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>289</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_d56d6f5f5</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2b58d945f</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>11</td>\n",
       "      <td>no</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>121</td>\n",
       "      <td>1369</td>\n",
       "      <td>16</td>\n",
       "      <td>121</td>\n",
       "      <td>4</td>\n",
       "      <td>1.777778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>1369</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id      v2a1  hacdor  rooms  hacapo  v14a  refrig  v18q  v18q1  \\\n",
       "0  ID_279628684  190000.0       0      3       0     1       1     0    NaN   \n",
       "1  ID_f29eb3ddd  135000.0       0      4       0     1       1     1    1.0   \n",
       "2  ID_68de51c94       NaN       0      8       0     1       1     0    NaN   \n",
       "3  ID_d671db89c  180000.0       0      5       0     1       1     1    1.0   \n",
       "4  ID_d56d6f5f5  180000.0       0      5       0     1       1     1    1.0   \n",
       "\n",
       "   r4h1  r4h2  r4h3  r4m1  r4m2  r4m3  r4t1  r4t2  r4t3  tamhog  tamviv  \\\n",
       "0     0     1     1     0     0     0     0     1     1       1       1   \n",
       "1     0     1     1     0     0     0     0     1     1       1       1   \n",
       "2     0     0     0     0     1     1     0     1     1       1       1   \n",
       "3     0     2     2     1     1     2     1     3     4       4       4   \n",
       "4     0     2     2     1     1     2     1     3     4       4       4   \n",
       "\n",
       "   escolari  rez_esc  hhsize  paredblolad  paredzocalo  paredpreb  pareddes  \\\n",
       "0        10      NaN       1            1            0          0         0   \n",
       "1        12      NaN       1            0            0          0         0   \n",
       "2        11      NaN       1            0            0          0         0   \n",
       "3         9      1.0       4            1            0          0         0   \n",
       "4        11      NaN       4            1            0          0         0   \n",
       "\n",
       "   paredmad  paredzinc  paredfibras  paredother  pisomoscer  pisocemento  \\\n",
       "0         0          0            0           0           1            0   \n",
       "1         1          0            0           0           0            0   \n",
       "2         1          0            0           0           1            0   \n",
       "3         0          0            0           0           1            0   \n",
       "4         0          0            0           0           1            0   \n",
       "\n",
       "   pisoother  pisonatur  pisonotiene  pisomadera  techozinc  techoentrepiso  \\\n",
       "0          0          0            0           0          0               1   \n",
       "1          0          0            0           1          1               0   \n",
       "2          0          0            0           0          1               0   \n",
       "3          0          0            0           0          1               0   \n",
       "4          0          0            0           0          1               0   \n",
       "\n",
       "   techocane  techootro  cielorazo  abastaguadentro  abastaguafuera  \\\n",
       "0          0          0          1                1               0   \n",
       "1          0          0          1                1               0   \n",
       "2          0          0          1                1               0   \n",
       "3          0          0          1                1               0   \n",
       "4          0          0          1                1               0   \n",
       "\n",
       "   abastaguano  public  planpri  noelec  coopele  sanitario1  sanitario2  \\\n",
       "0            0       1        0       0        0           0           1   \n",
       "1            0       1        0       0        0           0           1   \n",
       "2            0       1        0       0        0           0           1   \n",
       "3            0       1        0       0        0           0           1   \n",
       "4            0       1        0       0        0           0           1   \n",
       "\n",
       "   sanitario3  sanitario5  sanitario6  energcocinar1  energcocinar2  \\\n",
       "0           0           0           0              0              0   \n",
       "1           0           0           0              0              1   \n",
       "2           0           0           0              0              1   \n",
       "3           0           0           0              0              1   \n",
       "4           0           0           0              0              1   \n",
       "\n",
       "   energcocinar3  energcocinar4  elimbasu1  elimbasu2  elimbasu3  elimbasu4  \\\n",
       "0              1              0          1          0          0          0   \n",
       "1              0              0          1          0          0          0   \n",
       "2              0              0          1          0          0          0   \n",
       "3              0              0          1          0          0          0   \n",
       "4              0              0          1          0          0          0   \n",
       "\n",
       "   elimbasu5  elimbasu6  epared1  epared2  epared3  etecho1  etecho2  etecho3  \\\n",
       "0          0          0        0        1        0        1        0        0   \n",
       "1          0          0        0        1        0        0        1        0   \n",
       "2          0          0        0        1        0        0        0        1   \n",
       "3          0          0        0        0        1        0        0        1   \n",
       "4          0          0        0        0        1        0        0        1   \n",
       "\n",
       "   eviv1  eviv2  eviv3  dis  male  female  estadocivil1  estadocivil2  \\\n",
       "0      1      0      0    0     1       0             0             0   \n",
       "1      0      1      0    0     1       0             0             0   \n",
       "2      0      0      1    1     0       1             0             0   \n",
       "3      0      0      1    0     1       0             0             0   \n",
       "4      0      0      1    0     0       1             0             1   \n",
       "\n",
       "   estadocivil3  estadocivil4  estadocivil5  estadocivil6  estadocivil7  \\\n",
       "0             0             1             0             0             0   \n",
       "1             0             1             0             0             0   \n",
       "2             0             0             0             1             0   \n",
       "3             0             0             0             0             1   \n",
       "4             0             0             0             0             0   \n",
       "\n",
       "   parentesco1  parentesco2  parentesco3  parentesco4  parentesco5  \\\n",
       "0            1            0            0            0            0   \n",
       "1            1            0            0            0            0   \n",
       "2            1            0            0            0            0   \n",
       "3            0            0            1            0            0   \n",
       "4            0            1            0            0            0   \n",
       "\n",
       "   parentesco6  parentesco7  parentesco8  parentesco9  parentesco10  \\\n",
       "0            0            0            0            0             0   \n",
       "1            0            0            0            0             0   \n",
       "2            0            0            0            0             0   \n",
       "3            0            0            0            0             0   \n",
       "4            0            0            0            0             0   \n",
       "\n",
       "   parentesco11  parentesco12    idhogar  hogar_nin  hogar_adul  hogar_mayor  \\\n",
       "0             0             0  21eb7fcc1          0           1            0   \n",
       "1             0             0  0e5d7a658          0           1            1   \n",
       "2             0             0  2c7317ea8          0           1            1   \n",
       "3             0             0  2b58d945f          2           2            0   \n",
       "4             0             0  2b58d945f          2           2            0   \n",
       "\n",
       "   hogar_total dependency edjefe edjefa  meaneduc  instlevel1  instlevel2  \\\n",
       "0            1         no     10     no      10.0           0           0   \n",
       "1            1          8     12     no      12.0           0           0   \n",
       "2            1          8     no     11      11.0           0           0   \n",
       "3            4        yes     11     no      11.0           0           0   \n",
       "4            4        yes     11     no      11.0           0           0   \n",
       "\n",
       "   instlevel3  instlevel4  instlevel5  instlevel6  instlevel7  instlevel8  \\\n",
       "0           0           1           0           0           0           0   \n",
       "1           0           0           0           0           0           1   \n",
       "2           0           0           1           0           0           0   \n",
       "3           0           1           0           0           0           0   \n",
       "4           0           0           1           0           0           0   \n",
       "\n",
       "   instlevel9  bedrooms  overcrowding  tipovivi1  tipovivi2  tipovivi3  \\\n",
       "0           0         1      1.000000          0          0          1   \n",
       "1           0         1      1.000000          0          0          1   \n",
       "2           0         2      0.500000          1          0          0   \n",
       "3           0         3      1.333333          0          0          1   \n",
       "4           0         3      1.333333          0          0          1   \n",
       "\n",
       "   tipovivi4  tipovivi5  computer  television  mobilephone  qmobilephone  \\\n",
       "0          0          0         0           0            1             1   \n",
       "1          0          0         0           0            1             1   \n",
       "2          0          0         0           0            0             0   \n",
       "3          0          0         0           0            1             3   \n",
       "4          0          0         0           0            1             3   \n",
       "\n",
       "   lugar1  lugar2  lugar3  lugar4  lugar5  lugar6  area1  area2  age  \\\n",
       "0       1       0       0       0       0       0      1      0   43   \n",
       "1       1       0       0       0       0       0      1      0   67   \n",
       "2       1       0       0       0       0       0      1      0   92   \n",
       "3       1       0       0       0       0       0      1      0   17   \n",
       "4       1       0       0       0       0       0      1      0   37   \n",
       "\n",
       "   SQBescolari  SQBage  SQBhogar_total  SQBedjefe  SQBhogar_nin  \\\n",
       "0          100    1849               1        100             0   \n",
       "1          144    4489               1        144             0   \n",
       "2          121    8464               1          0             0   \n",
       "3           81     289              16        121             4   \n",
       "4          121    1369              16        121             4   \n",
       "\n",
       "   SQBovercrowding  SQBdependency  SQBmeaned  agesq  Target  \n",
       "0         1.000000            0.0      100.0   1849       4  \n",
       "1         1.000000           64.0      144.0   4489       4  \n",
       "2         0.250000           64.0      121.0   8464       4  \n",
       "3         1.777778            1.0      121.0    289       4  \n",
       "4         1.777778            1.0      121.0   1369       4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos el dataset original en una variable\n",
    "url_data_train = 'https://github.com/jbergamasco/DiploDatos2019/raw/master/ProyectoPobrezaCostaRica/DatasetPobCR_Train.csv'\n",
    "_ds = pd.read_csv(url_data_train)\n",
    "_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable_name</th>\n",
       "      <th>Variable_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v2a1</td>\n",
       "      <td>Monthly rent payment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hacdor</td>\n",
       "      <td>=1 Overcrowding by bedrooms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rooms</td>\n",
       "      <td>number of all rooms in the house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hacapo</td>\n",
       "      <td>=1 Overcrowding by rooms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v14a</td>\n",
       "      <td>=1 has bathroom in the household</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Variable_name              Variable_description\n",
       "0          v2a1              Monthly rent payment\n",
       "1        hacdor       =1 Overcrowding by bedrooms\n",
       "2         rooms  number of all rooms in the house\n",
       "3        hacapo          =1 Overcrowding by rooms\n",
       "4          v14a  =1 has bathroom in the household"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos el Dataset Fields en una variable, para tener la información de cada campo a mano\n",
    "# Si hay nueva metadata, cargarla posteriormente\n",
    "url_fields_info = 'https://github.com/jbergamasco/DiploDatos2019/raw/master/ProyectoPobrezaCostaRica/Dataset%20Fields.txt'\n",
    "_data_fields = pd.read_csv(url_fields_info, sep='\\t', encoding = 'ANSI')\n",
    "_data_fields.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuerden que la variable `Target` constituye nuestro objetivo original de predicción. Es la etiqueta de los datos de acuerdo al nivel de pobreza del hogar que habita cada individuo, según la siguiente escala o clases:\n",
    "\n",
    "1 = pobreza extrema<br>\n",
    "2 = pobreza moderada<br>\n",
    "3 = hogares vulnerables<br>\n",
    "4 = hogares no vulnerables<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "El dataset ya está **listo para trabajar!**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Aplicar Script de Curación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso implica aplicar el script que resultó del práctico anterior. También pueden adicionar campos calculados en base a otros atributos, según lo consideren pertinente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas eliminadas:  ['SQBescolari' 'SQBage' 'SQBhogar_total' 'SQBedjefe' 'SQBhogar_nin'\n",
      " 'SQBovercrowding' 'SQBdependency' 'SQBmeaned' 'agesq']\n",
      "Cantidad de familias sin caracteristicas comunes:  19\n",
      "Cantidad de familias sin caracteristicas comunes:  4\n"
     ]
    }
   ],
   "source": [
    "import clean\n",
    "\n",
    "clean_ds = clean.clean_datadrame(_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Análisis de Balance de Clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluar el balance de clases. Dejar plasmadas las decisiones que tomen al respecto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balanceo de clases jefes de hogar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ds[clean_ds['parentesco1']==1].groupby('Target')['Id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.svm import LinearSVC\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from collections import Counter\n",
    "\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_resampled_random_jef, y_resampled_random_jef = ros.fit_resample(clean_ds[clean_ds['parentesco1']==1].drop(['Id','idhogar','Target'], axis=1), clean_ds[clean_ds['parentesco1']==1]['Target'])\n",
    "print(sorted(Counter(y_resampled_random_jef).items()))\n",
    "\n",
    "X_resampled_smote_jef, y_resampled_smote_jef = SMOTE().fit_resample(clean_ds[clean_ds['parentesco1']==1].drop(['Id','idhogar','Target'], axis=1), clean_ds[clean_ds['parentesco1']==1]['Target'])\n",
    "print(sorted(Counter(y_resampled_smote_jef).items()))\n",
    "\n",
    "X_resampled_adasyn_jef, y_resampled_adasyn_jef = ADASYN().fit_resample(clean_ds[clean_ds['parentesco1']==1].drop(['Id','idhogar','Target'], axis=1), clean_ds[clean_ds['parentesco1']==1]['Target'])\n",
    "print(sorted(Counter(y_resampled_adasyn_jef).items()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balanceo de clases individuo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ds.groupby('Target')['Id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ds.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=0)\n",
    "X_resampled_random_ind, y_resampled_random_ind = ros.fit_resample(clean_ds.drop(['Target'], axis=1), clean_ds['Target'])\n",
    "print(sorted(Counter(y_resampled_random_ind).items()))\n",
    "data_oversampled_random_ind = pd.concat([pd.DataFrame(X_resampled_random_ind), pd.DataFrame(y_resampled_random_ind)], axis=1)\n",
    "data_oversampled_random_ind.columns = clean_ds.columns\n",
    "\n",
    "X_resampled_smote_ind, y_resampled_smote_ind = SMOTE().fit_resample(clean_ds.drop(['Target','Id','idhogar'], axis=1), clean_ds['Target'])\n",
    "print(sorted(Counter(y_resampled_smote_ind).items()))\n",
    "data_oversampled_smote_ind = pd.concat([pd.DataFrame(X_resampled_smote_ind), pd.DataFrame(y_resampled_smote_ind)], axis=1)\n",
    "data_oversampled_smote_ind.columns = clean_ds.drop(['Id','idhogar'], axis=1).columns\n",
    "\n",
    "X_resampled_adasyn_ind, y_resampled_adasyn_ind = ADASYN().fit_resample(clean_ds.drop(['Target','Id','idhogar'], axis=1), clean_ds['Target'])\n",
    "print(sorted(Counter(y_resampled_adasyn_ind).items()))\n",
    "data_oversampled_adasyn_ind = pd.concat([pd.DataFrame(X_resampled_adasyn_ind), pd.DataFrame(y_resampled_adasyn_ind)], axis=1)\n",
    "data_oversampled_adasyn_ind.columns = clean_ds.drop(['Id','idhogar'], axis=1).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Multicolinealidad Exacta y Variables Dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decisiones respecto a las variables con multicolinealidad perfecta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_multicollinearity(df, show=False):\n",
    "    dummy_list = [['parentesco1','parentesco2', 'parentesco3', 'parentesco4', 'parentesco5',\n",
    "           'parentesco6', 'parentesco7', 'parentesco8', 'parentesco9',\n",
    "           'parentesco10','parentesco11','parentesco12'],\n",
    "                 ['paredblolad','paredzocalo','paredpreb','pareddes','paredmad','paredzinc','paredfibras','paredother'],\n",
    "                  ['pisomoscer','pisocemento','pisoother','pisonatur','pisonotiene','pisomadera'],\n",
    "                  ['techozinc','techoentrepiso','techocane','techootro'],\n",
    "                  ['abastaguadentro','abastaguafuera','abastaguano'],\n",
    "                 ['public','planpri','noelec','coopele'],\n",
    "                  ['sanitario1','sanitario2','sanitario3','sanitario5','sanitario6'],\n",
    "                  ['energcocinar1','energcocinar2','energcocinar3','energcocinar4'],\n",
    "                  ['elimbasu1','elimbasu2','elimbasu3','elimbasu4','elimbasu5','elimbasu6'],\n",
    "                  ['epared1','epared2','epared3'],\n",
    "                  ['etecho1','etecho2','etecho3'],\n",
    "                  ['eviv1','eviv2','eviv3'],\n",
    "                  ['male','female'],\n",
    "                  ['estadocivil1','estadocivil2','estadocivil3','estadocivil4','estadocivil5','estadocivil6','estadocivil7'],\n",
    "                  ['instlevel1','instlevel2','instlevel3','instlevel4','instlevel5','instlevel6','instlevel7','instlevel8','instlevel9'],\n",
    "                  ['tipovivi1','tipovivi2','tipovivi3','tipovivi4','tipovivi5'],\n",
    "                  ['region_central','region_chorotega','region_pacifico_central','region_brunca','region_huetar_atlantica','region_huetar_norte'],\n",
    "                  ['zona_urbana','zona_rural']\n",
    "                 ]\n",
    "\n",
    "    drop_list = []\n",
    "    for dummy in dummy_list:\n",
    "\n",
    "        k = dummy[0]\n",
    "        colin =  pd.DataFrame(data=df[k], columns=[k])\n",
    "        colin['suma'] = df[df[dummy].columns.difference([k])].sum(axis=1)\n",
    "        corr = colin.corr(method='spearman')\n",
    "        if show:\n",
    "            print(k)\n",
    "            print(corr)\n",
    "        drop_list.append(dummy[-1])\n",
    "        #plt.figure(figsize = (8,5))\n",
    "        #sns.heatmap(corr, annot=True,fmt=\"f\", vmin=-1, vmax=1)\n",
    "        \n",
    "    return df.drop(drop_list, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Clasificación a Nivel Hogar o a Nivel Individuo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto, deberán elegir si trabajar el dataset por individuo o por hogar.\n",
    "\n",
    "En caso de elegir hacerlo por hogar, crear un nuevo dataset, `_ds_hogar`, manteniendo únicamente los individuos jefe de hogar (```parentesco1 == 1```) y los atributos que se repitan por hogar. Para aquellos atributos individuales, crear medidas sintéticas por hogar relevantes (suma, promedio simple, promedio ponderado, máximo, mínimo, etc.).\n",
    "\n",
    "Por ejemplo, para la escolaridad, mantendría la de la persona jefa de hogar y crearía otra (u otras) que indiquen la escolaridad promedio o la escolaridad promedio por adulto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ds_individuo = _ds\n",
    "_ds_hogar = _ds[_ds['parentesco1'] == 1]\n",
    "_ds_hogar.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Normalización de Atributos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicar al dataset la normalización de atributos que consideren adecuada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pueden utilizar los siguientes métodos, por ejemplo:\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def normalizar(df, scaler):\n",
    "    fields = ['monthly_rent','hacdor','rooms','hogar_nin','hogar_adul','qmobilephone','hogar_mayor','v18q1','r4h1','r4h2','r4m1','r4m2','tamhog','tamviv','escolari','bedrooms','overcrowding','age']\n",
    "    for col in fields:\n",
    "        df[col] = scaler.fit_transform(df[[col]])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Report in CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Mezca Aleatória y División en Train/Test\n",
    "\n",
    "Primeramente, deberán mezclar los datos aleatoriamente. Luego, para dividir en Train/Test el dataset, aplicar el split utilizando un 20% de datos para este último.\n",
    "\n",
    "En este punto, deberán obtener cuatro conjuntos de datos, para ambos datasets: ```X_train```, ```X_test```, ```y_train``` y ```y_test```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_divide_train_test(df):\n",
    "    _ds_shuff = shuffle(df) \n",
    "    return train_test_split(_ds_shuff.drop(['Target'], axis=1), _ds_shuff['Target'], test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Aplicación de Modelos de Clasificación en individuos Random oversampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando los datos de train y test obtenidos, se aplicarán diferentes modelos de clasificación para el dataset seleccionado, ya sea `_ds_individuo` o `_ds_hogar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multicollinearity_oversampled_random_ind = drop_multicollinearity(data_oversampled_random_ind)\n",
    "minmax = preprocessing.MinMaxScaler()\n",
    "oversampled_random_ind = normalizar(multicollinearity_oversampled_random_ind,minmax )\n",
    "X_train, X_test, y_train, y_test= mix_divide_train_test(oversampled_random_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se aplicará un clasificador SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algunos kernels son: \"rbf\",\"sigmoid\",\"poly\"\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel=\"sigmoid\", random_state=0, class_weight=None)\n",
    "model.fit(X_train.drop(['Id','idhogar'], axis=1), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se calcula la exactitud para ambos conjuntos, train y test:\n",
    "import store\n",
    "y_pred_train =   model.predict(X_train.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "print(\"Exactitud del algoritmo para conjunto de entrenamiento: %.2f\" % accuracy_train)\n",
    "\n",
    "y_pred_test =   model.predict(X_test.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Exactitud del algoritmo para conjunto de validación: %.2f\" % accuracy_test)\n",
    "save_mlflow(\"SVC\",classification_report(y_test, y_pred_test, output_dict=True), model.get_params(), {\"train_type\": \"data_oversampled_random_ind\"})\n",
    "report = classification_report(y_test, y_pred_test)\n",
    "\n",
    "print(report, end=\"\\n\\n\")\n",
    "print(\"Accuracy: %0.4f\" % accuracy_score(y_test, y_pred_test))\n",
    "print(\"================================================\", end=\"\\n\\n\")\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(y_test, y_pred_test),\n",
    "                      classes=('0', '1'), title=\"Matriz de confusión\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mlflow(model_name, clasification_report, model_params, extra_param_dic):\n",
    "    experiment_name = \"PobrezaCostaRica\"\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Log model hiperparameters first\n",
    "        mlflow.log_param('model', 'SVC')\n",
    "        mlflow.log_param('clasification_report', clasification_report)\n",
    "        mlflow.log_param('model_params', model_params)\n",
    "        mlflow.log_param('extra_param_dic', extra_param_dic)\n",
    "\n",
    "        #print(\"*** Dev loss: {} - accuracy: {}\".format(loss, accuracy))\n",
    "        #mlflow.log_metric('dev-loss', loss)\n",
    "        #mlflow.log_metric('dev-accuracy', accuracy)\n",
    "\n",
    "        #loss, accuracy = model.evaluate(train_ds)\n",
    "        #print(\"*** Train loss: {} - accuracy: {}\".format(loss, accuracy))\n",
    "        #mlflow.log_metric('train-loss', loss)\n",
    "        #mlflow.log_metric('train-accuracy', accuracy)\n",
    "\n",
    "        #mlflow.log_metric('train-accuracy', accuracy)\n",
    "        #mlflow.log_metric('train-accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se aplicará un Random Forest para clasificar nuestro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(random_state=2)\n",
    "model.fit(X_train.drop(['Id','idhogar'], axis=1), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se calcula la exactitud para ambos conjuntos, train y test:\n",
    "\n",
    "y_pred_train =   model.predict(X_train.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "print(\"Exactitud del algoritmo para conjunto de entrenamiento: %.2f\" % accuracy_train)\n",
    "\n",
    "y_pred_test =   model.predict(X_test.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Exactitud del algoritmo para conjunto de validación: %.2f\" % accuracy_test)\n",
    "store.classification_report_csv(\"RandomForestClassifier\",classification_report(y_test, y_pred_test, output_dict=True), model.get_params(), {\"train_type\": \"data_oversampled_random_ind\", \"date\": datetime.datetime.now()})\n",
    "print(classification_report(y_test, y_pred_test), end=\"\\n\\n\")\n",
    "print(\"Accuracy: %0.4f\" % accuracy_score(y_test, y_pred_test))\n",
    "print(\"================================================\", end=\"\\n\\n\")\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(y_test, y_pred_test),\n",
    "                      classes=('0', '1'), title=\"Matriz de confusión\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se entrenará una red neuronal para clasificar el dataset. Pueden usar otra librería si lo desean, por ejemplo, Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1, max_iter=5000)\n",
    "model.fit(X_train.drop(['Id','idhogar'], axis=1), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train =   model.predict(X_train.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "print(\"Exactitud del algoritmo para conjunto de entrenamiento: %.2f\" % accuracy_train)\n",
    "\n",
    "y_pred_test =   model.predict(X_test.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Exactitud del algoritmo para conjunto de validación: %.2f\" % accuracy_test)\n",
    "store.classification_report_csv(\"MLPClassifier\",classification_report(y_test, y_pred_test, output_dict=True), model.get_params(), {\"train_type\": \"data_oversampled_random_ind\", \"date\": datetime.datetime.now().strftime(\"%d/%m/%Y\")})\n",
    "print(classification_report(y_test, y_pred_test), end=\"\\n\\n\")\n",
    "print(\"Accuracy: %0.4f\" % accuracy_score(y_test, y_pred_test))\n",
    "print(\"================================================\", end=\"\\n\\n\")\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(y_test, y_pred_test),\n",
    "                      classes=('0', '1'), title=\"Matriz de confusión\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Librería Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.core import Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.001), \n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, batch_size=128,\n",
    "          epochs=2, verbose=1, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mayor información, consultar los siguientes links:\n",
    "- https://keras.io/optimizers/\n",
    "- https://keras.io/losses/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se calcula la exactitud para ambos conjuntos, train y test:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Clasificador por Votos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial',\n",
    "                          random_state=1)\n",
    "clf1.fit(X_train.drop(['Id','idhogar'], axis=1), y_train)\n",
    "y_pred_train =   clf1.predict(X_train.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "print(\"Exactitud del algoritmo para conjunto de entrenamiento: %.2f\" % accuracy_train)\n",
    "y_pred_test =   clf1.predict(X_test.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Exactitud del algoritmo para conjunto de validación: %.2f\" % accuracy_test)\n",
    "\n",
    "\n",
    "clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "clf2.fit(X_train.drop(['Id','idhogar'], axis=1), y_train)\n",
    "y_pred_train =   clf2.predict(X_train.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "print(\"Exactitud del algoritmo para conjunto de entrenamiento: %.2f\" % accuracy_train)\n",
    "y_pred_test =   clf2.predict(X_test.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Exactitud del algoritmo para conjunto de validación: %.2f\" % accuracy_test)\n",
    "\n",
    "clf3 = GaussianNB()\n",
    "clf3.fit(X_train.drop(['Id','idhogar'], axis=1), y_train)\n",
    "\n",
    "y_pred_train =   clf3.predict(X_train.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "print(\"Exactitud del algoritmo para conjunto de entrenamiento: %.2f\" % accuracy_train)\n",
    "y_pred_test =   clf3.predict(X_test.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Exactitud del algoritmo para conjunto de validación: %.2f\" % accuracy_test)\n",
    "\n",
    "\n",
    "\n",
    "eclf1 = VotingClassifier(estimators=[\n",
    "        ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n",
    "eclf1 = eclf1.fit(X_train.drop(['Id','idhogar'], axis=1), y_train)\n",
    "print(eclf1.predict(X_train.drop(['Id','idhogar'], axis=1)))\n",
    "y_pred_train =   eclf1.predict(X_train.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "print(\"Exactitud del algoritmo para conjunto de entrenamiento: %.2f\" % accuracy_train)\n",
    "y_pred_test =   eclf1.predict(X_test.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Exactitud del algoritmo para conjunto de validación: %.2f\" % accuracy_test)\n",
    "\n",
    "np.array_equal(eclf1.named_estimators_.lr.predict(X_train.drop(['Id','idhogar'], axis=1)),\n",
    "               eclf1.named_estimators_['lr'].predict(X_train.drop(['Id','idhogar'], axis=1)))\n",
    "\n",
    "eclf2 = VotingClassifier(estimators=[\n",
    "        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
    "        voting='soft')\n",
    "eclf2 = eclf2.fit(X_train.drop(['Id','idhogar'], axis=1), y_train)\n",
    "print(eclf2.predict(X_train.drop(['Id','idhogar'], axis=1)))\n",
    "y_pred_train =   eclf2.predict(X_train.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "print(\"Exactitud del algoritmo para conjunto de entrenamiento: %.2f\" % accuracy_train)\n",
    "y_pred_test =   eclf2.predict(X_test.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Exactitud del algoritmo para conjunto de validación: %.2f\" % accuracy_test)\n",
    "\n",
    "eclf3 = VotingClassifier(estimators=[\n",
    "       ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
    "       voting='soft', weights=[2,1,1],\n",
    "       flatten_transform=True)\n",
    "eclf3 = eclf3.fit(X_train.drop(['Id','idhogar'], axis=1), y_train)\n",
    "print(eclf3.predict(X_train.drop(['Id','idhogar'], axis=1)))\n",
    "y_pred_train =   eclf3.predict(X_train.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "print(\"Exactitud del algoritmo para conjunto de entrenamiento: %.2f\" % accuracy_train)\n",
    "y_pred_test =   eclf3.predict(X_test.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Exactitud del algoritmo para conjunto de validación: %.2f\" % accuracy_test)\n",
    "\n",
    "store.classification_report_csv(\"VotingClassifier\",classification_report(y_test, y_pred_test, output_dict=True), model.get_params(), {\"train_type\": \"data_oversampled_random_ind\", \"date\": datetime.datetime.now()})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Aplicación de Modelos de Clasificación en individuos SMOTE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando los datos de train y test obtenidos, se aplicarán diferentes modelos de clasificación para el dataset seleccionado, ya sea `_ds_individuo` o `_ds_hogar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multicollinearity_oversampled_smote_ind = drop_multicollinearity(data_oversampled_smote_ind)\n",
    "minmax = preprocessing.MinMaxScaler()\n",
    "oversampled_smote_ind = normalizar(multicollinearity_oversampled_smote_ind,minmax )\n",
    "X_train, X_test, y_train, y_test= mix_divide_train_test(oversampled_smote_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se aplicará un clasificador SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algunos kernels son: \"rbf\",\"sigmoid\",\"poly\"\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel=\"sigmoid\", random_state=0, class_weight=None)\n",
    "model.fit(X_train.drop(['Id','idhogar'], axis=1), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se calcula la exactitud para ambos conjuntos, train y test:\n",
    "\n",
    "y_pred_train =   model.predict(X_train.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "print(\"Exactitud del algoritmo para conjunto de entrenamiento: %.2f\" % accuracy_train)\n",
    "\n",
    "y_pred_test =   model.predict(X_test.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Exactitud del algoritmo para conjunto de validación: %.2f\" % accuracy_test)\n",
    "\n",
    "report = classification_report(y_test, y_pred_test)\n",
    "\n",
    "store.classification_report_csv(\"SVC\",classification_report(y_test, y_pred_test, output_dict=True), model.get_params(), {\"train_type\": \"data_oversampled_random_ind\", \"date\": datetime.datetime.now()})\n",
    "\n",
    "print(report, end=\"\\n\\n\")\n",
    "print(\"Accuracy: %0.4f\" % accuracy_score(y_test, y_pred_test))\n",
    "print(\"================================================\", end=\"\\n\\n\")\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(y_test, y_pred_test),\n",
    "                      classes=('0', '1'), title=\"Matriz de confusión\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se aplicará un Random Forest para clasificar nuestro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(random_state=2)\n",
    "model.fit(X_train.drop(['Id','idhogar'], axis=1), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se calcula la exactitud para ambos conjuntos, train y test:\n",
    "\n",
    "y_pred_train =   model.predict(X_train.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "print(\"Exactitud del algoritmo para conjunto de entrenamiento: %.2f\" % accuracy_train)\n",
    "\n",
    "y_pred_test =   model.predict(X_test.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Exactitud del algoritmo para conjunto de validación: %.2f\" % accuracy_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_test), end=\"\\n\\n\")\n",
    "print(\"Accuracy: %0.4f\" % accuracy_score(y_test, y_pred_test))\n",
    "print(\"================================================\", end=\"\\n\\n\")\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(y_test, y_pred_test),\n",
    "                      classes=('0', '1'), title=\"Matriz de confusión\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se entrenará una red neuronal para clasificar el dataset. Pueden usar otra librería si lo desean, por ejemplo, Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1, max_iter=5000)\n",
    "model.fit(X_train.drop(['Id','idhogar'], axis=1), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train =   model.predict(X_train.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "print(\"Exactitud del algoritmo para conjunto de entrenamiento: %.2f\" % accuracy_train)\n",
    "\n",
    "y_pred_test =   model.predict(X_test.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Exactitud del algoritmo para conjunto de validación: %.2f\" % accuracy_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_test), end=\"\\n\\n\")\n",
    "print(\"Accuracy: %0.4f\" % accuracy_score(y_test, y_pred_test))\n",
    "print(\"================================================\", end=\"\\n\\n\")\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(y_test, y_pred_test),\n",
    "                      classes=('0', '1'), title=\"Matriz de confusión\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Librería Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.core import Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.001), \n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, batch_size=128,\n",
    "          epochs=2, verbose=1, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mayor información, consultar los siguientes links:\n",
    "- https://keras.io/optimizers/\n",
    "- https://keras.io/losses/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se calcula la exactitud para ambos conjuntos, train y test:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Clasificador por Votos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial',\n",
    "                          random_state=1)\n",
    "clf1.fit(X_train.drop(['Id','idhogar'], axis=1), y_train)\n",
    "y_pred_train =   clf1.predict(X_train.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "print(\"Exactitud del algoritmo para conjunto de entrenamiento: %.2f\" % accuracy_train)\n",
    "y_pred_test =   clf1.predict(X_test.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Exactitud del algoritmo para conjunto de validación: %.2f\" % accuracy_test)\n",
    "\n",
    "\n",
    "clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "clf2.fit(X_train.drop(['Id','idhogar'], axis=1), y_train)\n",
    "y_pred_train =   clf2.predict(X_train.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "print(\"Exactitud del algoritmo para conjunto de entrenamiento: %.2f\" % accuracy_train)\n",
    "y_pred_test =   clf2.predict(X_test.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Exactitud del algoritmo para conjunto de validación: %.2f\" % accuracy_test)\n",
    "\n",
    "clf3 = GaussianNB()\n",
    "clf3.fit(X_train.drop(['Id','idhogar'], axis=1), y_train)\n",
    "\n",
    "y_pred_train =   clf3.predict(X_train.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "print(\"Exactitud del algoritmo para conjunto de entrenamiento: %.2f\" % accuracy_train)\n",
    "y_pred_test =   clf3.predict(X_test.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Exactitud del algoritmo para conjunto de validación: %.2f\" % accuracy_test)\n",
    "\n",
    "\n",
    "\n",
    "eclf1 = VotingClassifier(estimators=[\n",
    "        ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n",
    "eclf1 = eclf1.fit(X_train.drop(['Id','idhogar'], axis=1), y_train)\n",
    "print(eclf1.predict(X_train.drop(['Id','idhogar'], axis=1)))\n",
    "y_pred_train =   eclf1.predict(X_train.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "print(\"Exactitud del algoritmo para conjunto de entrenamiento: %.2f\" % accuracy_train)\n",
    "y_pred_test =   eclf1.predict(X_test.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Exactitud del algoritmo para conjunto de validación: %.2f\" % accuracy_test)\n",
    "\n",
    "np.array_equal(eclf1.named_estimators_.lr.predict(X_train.drop(['Id','idhogar'], axis=1)),\n",
    "               eclf1.named_estimators_['lr'].predict(X_train.drop(['Id','idhogar'], axis=1)))\n",
    "\n",
    "eclf2 = VotingClassifier(estimators=[\n",
    "        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
    "        voting='soft')\n",
    "eclf2 = eclf2.fit(X_train.drop(['Id','idhogar'], axis=1), y_train)\n",
    "print(eclf2.predict(X_train.drop(['Id','idhogar'], axis=1)))\n",
    "y_pred_train =   eclf2.predict(X_train.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "print(\"Exactitud del algoritmo para conjunto de entrenamiento: %.2f\" % accuracy_train)\n",
    "y_pred_test =   eclf2.predict(X_test.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Exactitud del algoritmo para conjunto de validación: %.2f\" % accuracy_test)\n",
    "\n",
    "eclf3 = VotingClassifier(estimators=[\n",
    "       ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
    "       voting='soft', weights=[2,1,1],\n",
    "       flatten_transform=True)\n",
    "eclf3 = eclf3.fit(X_train.drop(['Id','idhogar'], axis=1), y_train)\n",
    "print(eclf3.predict(X_train.drop(['Id','idhogar'], axis=1)))\n",
    "y_pred_train =   eclf3.predict(X_train.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "print(\"Exactitud del algoritmo para conjunto de entrenamiento: %.2f\" % accuracy_train)\n",
    "y_pred_test =   eclf3.predict(X_test.drop(['Id','idhogar'], axis=1))\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Exactitud del algoritmo para conjunto de validación: %.2f\" % accuracy_test)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Selección del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Selección y Descripción de Hipótesis\n",
    "\n",
    "Describir el problema y la hipótesis del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Selección de Regularizador\n",
    "\n",
    " ¿Utilizaron algún regularizador?¿Cuál?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3. Selección de Función de Costo\n",
    "\n",
    "¿Cuál fue la función de costo utilizada?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4. Justificación de las Selecciones\n",
    "\n",
    "A continuación, se justifican las elecciones previas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Selección de Parámetros y Métricas Sobre el Conjunto de Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la selección de hiperparámetros, pueden utilizar GridSearch. Además, deben calcular las métricas solicitadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para la búsqueda de los mejores parámetros, por ejemplo de logistic regression, pueden usar:\n",
    "\n",
    "exploring_params = {\n",
    "        'params': [], # Lista de parámetros a explorar\n",
    "        }\n",
    "\n",
    "model = # Especificar modelo\n",
    "n_cross_val =   # Seleccionar folds\n",
    "scoring = 'roc_auc'\n",
    "model = GridSearchCV(m, exploring_params, cv=n_cross_val, scoring=scoring)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Mejor conjunto de parámetros:\")\n",
    "print(model.best_params_, end=\"\\n\\n\")\n",
    "print()\n",
    "print(\"Puntajes de la grilla:\", end=\"\\n\\n\")\n",
    "means = model.cv_results_['mean_test_score']\n",
    "stds = model.cv_results_['std_test_score'])\n",
    "print()\n",
    "print(\"Reporte de clasificación para el mejor clasificador (sobre conjunto de evaluación):\", end=\"\\n\\n\")\n",
    "y_true, y_pred = y_test, model.predict(X_test)\n",
    "print(classification_report(y_true, y_pred), end=\"\\n\\n\")\n",
    "\n",
    "print(\"================================================\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las métricas solicitadas son: accuracy_score, confusion_matrix, classification_report, roc_curve, auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
